{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Programming for Language Technologists\n",
    "Artur Kulmizev, Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-requisites:\n",
    "\n",
    "For this lecture, we'll be working with several Python libraries. A good way to keep track of the libraries you may use for various projects is to use virtual environments. You can do this in Python by first installing the `virtualenv` library:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`$ pip install --user virtualenv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the installation, you can create a virtual environment as follows:\n",
    "\n",
    "`$ virtualenv web_scraping`\n",
    "\n",
    "After doing this, a directory named `web_scraping` will be created in your local directory. This will hold all the necessary packages that you'll be working with on this project. \n",
    "\n",
    "To load a virtual env, issue the following command:\n",
    "\n",
    "`source web_scraping/bin/activate`\n",
    "\n",
    "After loading an env, you should its name at the beginning of your shell prompt:\n",
    "\n",
    "`(web_scraping)$`\n",
    "\n",
    "Once you're inside an environment, you can install packages as follows:\n",
    "\n",
    "`$ pip install --user beautifulsoup4 selenium`\n",
    "\n",
    "After the packages have completed installing, their installation will exist within this folder, isolated from the rest of the system. After deactivating the environment, your Python settings will revert to how they were before. To deactivate an environment, you can simply run `(web_scraping)$ deactivate`. \n",
    "\n",
    "#### Anaconda\n",
    "\n",
    "Anaconda is a Python distribution that comes bundled with many essential data science and machine learning packages. It can be downloaded [here](https://www.anaconda.com/distribution/#download-section). It is recommended that you install Anaconda on your personal computers since you'll be working with many relevant packages that it includes. Anaconda also features `conda`, which is a package manager and environment manager in-one. You can create and activate a `conda` virtual environment like so:\n",
    "\n",
    "`$ conda create -n web_scraping`\n",
    "`$ source activate web_scraping`\n",
    "\n",
    "Once inside the environment, you can also use `conda` to install packages:\n",
    "\n",
    "`(web_scraping) $ conda install beautifulsoup4`\n",
    "\n",
    "To deactivate the environment, simply do:\n",
    "\n",
    "`(web_scraping) $ deactivate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1: Web Scraping\n",
    "\n",
    "### _What is web scraping?_\n",
    "\n",
    "Web scraping refers to the practice of writing computer programs that can extract data from web pages. People typically scrape the web for images or text, but other types of data - like geolocation and demographics - are also very prevalent. \n",
    "\n",
    "### _How is web scraping relevant to LT?_\n",
    "\n",
    "As language technologists-in-training, so far you've mostly worked with curated corpus text. This type of text (whether annotated or not) is immensely useful for training machine learning-based NLP systems, but is often very limited in terms of access and scope. \n",
    "\n",
    "Say you wanted to use Twitter data to investigate dialect differences across various regions of Switzerland. You'll likely need a dataset or corpus of Tweets, along with user-provided geolocation or something of the sort. Perhaps there is an existing dataset you could use, but, in most cases, there likely isn't. Similarly, what if you did manage to get your hands on a perfect corpus, but it only collected Tweets from 2010? Given the rapid pace of language change on the internet, it is unlikely that the data would be sufficiently up-to-date for your purposes. In such cases, you'll probably need to get the data yourself. \n",
    "\n",
    "### _How can we do it?_\n",
    "\n",
    "Typically, when you've determined that you need to acquire your own data from the internet, there are two scenarios that arise. \n",
    "\n",
    "#### Easy Scenario: the website provides an API:\n",
    "\n",
    "API stands for Application Programming Interface. APIs are essentially enterprise-provided tools and guidelines that help users programatically interact with their services. Instead of attempting to navigate the maze of data structures in a particular website from scratch, users can use a website's API to make the process significantly easier - most of the work has already been done for them. For example, [Twitter offers an API](https://developer.twitter.com/en/docs) that specifies how one can retrieve Tweets from a specific time/date, post one's own Tweets, and find relevant information, like geolocation, etc. This can then be used to write programs in the user's language of choice, say, Python. \n",
    "\n",
    "Unfortunately, APIs are often only provided by enterprise systems like Twitter, Google, and Facebook. As such, someone looking to crawl disparate sources of information (e.g. English-language blog posts about the Iraq War circa 2001-2004) will need to get creative.\n",
    "\n",
    "#### Hard Scenario: the website provides no API:\n",
    "\n",
    "In the case that you don't have access to a website's API, you'll probably have to interact with webpages directly. Webpages are built upon HTML, which stands for Hypertext Markup Language. Scraping is typically done by processing lots of pages, parsing their HTML structure, and returning relevant bits of that structure, like paragraphs of text. Though this might seem daunting, there are a multitude of libraries that can help you make this process relatively painless. For the sake of this tutorial, we'll focus on these. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HTML review\n",
    "\n",
    "A very simple HTML page might look something like this.\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "    <h1>My First Heading</h1>\n",
    "    <p>My first paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html> \n",
    "```\n",
    "\n",
    "HTML documents (just webpages) consist of elements that tell browsers how to display content. Elements are typically things like headings, paragraphs, lists, that can be used to format, structure, and stylize webpages. (Most) elements have appear between between tags, which specify their beginning and ends, like so: `<h1>This is a heading</h1>`. A typical HTML document consists of the following structure:\n",
    "\n",
    "* a `<!DOCTYPE html>` declaration, which specifies to the browser that the document is indeed an HTML5 document.\n",
    "* an `<html>` element that contains the entire page's contents.\n",
    "* a `<head>` element that contains information and metadata about the page, like its title, etc.\n",
    "* a `<body>` element that contains the main part of the page: its structure, headings, text, etc.\n",
    "\n",
    "The goal of web scraping is to extract information that is structured in this format. It is important to note that very, very few webpages have simple HTML-parsable formats. Also, HTML isn't the only language at play in web development - far from it. A typical webpage might consist of a complicated interplay of various languages: HTML, JavaScript, Ruby, etc. This is what makes scraping a challenging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How can we parse HTML with Python?\n",
    "\n",
    "Naively? Regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My First Heading', 'My Second Heading']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "my_page = '''\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "    <h1>My First Heading</h1>\n",
    "    <p>My first paragraph.</p>\n",
    "\n",
    "    <h1>My Second Heading</h1>\n",
    "    <p>My second paragraph.</p>\n",
    "    \n",
    "</body>\n",
    "</html> \n",
    "'''\n",
    "\n",
    "#Search for the title of the page\n",
    "\n",
    "title_search = re.compile(r'<h1.*?>(.+?)</h1>')\n",
    "title = title_search.findall(my_page)\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works well for very simple, toy HTML pages. With more complex HTML pages, the code becomes messy and unpredictable, so regex is a waste of time. \n",
    "\n",
    "Better to use an HTML parser, like Beautiful Soup. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html>\n",
      "<html>\n",
      " <head>\n",
      "  <title>\n",
      "   My Title\n",
      "  </title>\n",
      " </head>\n",
      " <body>\n",
      "  <h1>\n",
      "   My First Heading\n",
      "  </h1>\n",
      "  <p>\n",
      "   My first paragraph.\n",
      "  </p>\n",
      "  <h1>\n",
      "   My Second Heading\n",
      "  </h1>\n",
      "  <p>\n",
      "   My second paragraph.\n",
      "  </p>\n",
      " </body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# We can call BeautifulSoup on our page to parse the document\n",
    "\n",
    "soup = BeautifulSoup(my_page, \"html.parser\")\n",
    "\n",
    "# This gives us a parsed object that we can interact with\n",
    "\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>My Title</title>\n",
      "My Title\n"
     ]
    }
   ],
   "source": [
    "# We can extract any element out of the document...\n",
    "\n",
    "print(soup.title)\n",
    "\n",
    "# ... as well as its corresponding text\n",
    "\n",
    "print(soup.title.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p>My first paragraph.</p>\n",
      "[<p>My first paragraph.</p>, <p>My second paragraph.</p>]\n",
      "My first paragraph.\n",
      "My second paragraph.\n"
     ]
    }
   ],
   "source": [
    "# We can use soup to find the first instance of an element...\n",
    "\n",
    "print(soup.p)\n",
    "\n",
    "# ... or all instances of an element...\n",
    "\n",
    "print(soup.find_all(\"p\"))\n",
    "\n",
    "# ... and their corresponding text\n",
    "\n",
    "for paragraph in soup.find_all(\"p\"):\n",
    "    print(paragraph.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to parse live web pages? We can do the following using `urllib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Miryam de Lhoneux\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "\n",
    "site_string = \"https://cl.lingfil.uu.se/~miryam/\"\n",
    "\n",
    "miryam_site = urllib.request.urlopen(site_string)\n",
    "soup2 = BeautifulSoup(miryam_site, \"html.parser\")\n",
    "\n",
    "print(soup2.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptions\n",
    "\n",
    "Let's try loading the webpages of several computational linguists at Uppsala and see what they call their page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gongbo Tang\n",
      " Miryam de Lhoneux\n",
      "Sara Stymne\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "HTTP Error 404: Not Found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-75870542de95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"gongbo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"miryam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sara\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"artur\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0msite_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"https://cl.lingfil.uu.se/~{name}/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcl_site\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msite_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcl_site\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 404: Not Found"
     ]
    }
   ],
   "source": [
    "for name in [\"gongbo\", \"miryam\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    cl_site = urllib.request.urlopen(site_string)\n",
    "    soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "    print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems our code ran for the first three names, but failed on mine. The last site caused `urrlib` to raise an `HTTPError`, which was raised here: `--> 649         raise HTTPError(req.full_url, code, msg, hdrs, fp)`. Let's look at some other types of exceptions in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-d8dc9b97521f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ZeroDivisionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;36m12\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# ZeroDivisionError\n",
    "\n",
    "12/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'variable_we_did_not_define' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c71dd5def572>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# NameError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mvariable_we_did_not_define\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'variable_we_did_not_define' is not defined"
     ]
    }
   ],
   "source": [
    "# NameError \n",
    "\n",
    "variable_we_did_not_define + 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-64659fcd19b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TypeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;36m12\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"cat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "# TypeError\n",
    "\n",
    "12 + \"cat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can handle exceptions using `try` and `except` statements. `try` tells Python to try running your code, with the expectation that something might go awry. `except` tells Python that, when something does go awry, you have code to handle the situation accordingly. Let's try them in our code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gongbo Tang\n",
      " Miryam de Lhoneux\n",
      "Sara Stymne\n",
      "Site not found.\n"
     ]
    }
   ],
   "source": [
    "for name in [\"gongbo\", \"miryam\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "        print(soup.title.string)\n",
    "    except:\n",
    "        print(\"Site not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more specificity, we can include the exact type of exception we're hoping to catch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gongbo Tang\n",
      " Miryam de Lhoneux\n",
      "Sara Stymne\n",
      "Site not found.\n"
     ]
    }
   ],
   "source": [
    "from urllib.error import HTTPError\n",
    "\n",
    "for name in [\"gongbo\", \"miryam\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "        print(soup.title.string)\n",
    "    except HTTPError:\n",
    "        print(\"Site not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `else` statements to have more control over the program. Like in `if-else` statments, `try-else` works as a fallback to when an exception is *not* caught. For the purposes of this code, then, we can just move `print(soup.title.string)` inside `else`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gongbo Tang\n",
      " Miryam de Lhoneux\n",
      "Sara Stymne\n",
      "Site not found.\n"
     ]
    }
   ],
   "source": [
    "for name in [\"gongbo\", \"miryam\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "    except HTTPError:\n",
    "        print(\"Site not found.\")\n",
    "    else:\n",
    "        print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use `finally` statements to execute code regardless of the status of the exception checker. This code will always execute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gongbo Tang\n",
      "Finished processing page for gongbo.\n",
      " Miryam de Lhoneux\n",
      "Finished processing page for miryam.\n",
      "Sara Stymne\n",
      "Finished processing page for sara.\n",
      "*SITE NOT FOUND.*\n",
      "Finished processing page for artur.\n"
     ]
    }
   ],
   "source": [
    "for name in [\"gongbo\", \"miryam\", \"sara\", \"artur\"]:\n",
    "    site_string = f\"https://cl.lingfil.uu.se/~{name}/\"\n",
    "    try:\n",
    "        cl_site = urllib.request.urlopen(site_string)\n",
    "        soup = BeautifulSoup(cl_site, \"html.parser\")\n",
    "    except HTTPError:\n",
    "        print(\"*SITE NOT FOUND.*\")\n",
    "    else:\n",
    "        print(soup.title.string)\n",
    "    finally:\n",
    "        print(f\"Finished processing page for {name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more challenging example: scraping presidential speeches\n",
    "\n",
    "So far, we've been working with fairly straightforward HTML pages that are easy to scrape. In most situations, this won't be the case. For the next module, we will be working with the text transcripts of all US presidential speeches. However, there is no corpus that collects this information. Luckily, The Miller Center (a University of Virginia research center that studies the history of the US presidency) has made all popular presidential speeches public at [this address](https://millercenter.org/the-presidency/presidential-speeches). We can use these, but since the raw text of these speeches is not available for download, we will have to scrape the necessary content ourselves. \n",
    "\n",
    "Judging from the site, it appears that actual transcripts live on separate pages, which are structured like this: https://millercenter.org/the-presidency/presidential-speeches/january-8-2020-statement-iran. It is these pages that we will need to parse using BeautifulSoup. However, in order to do so, we need to know the links to each individual page. Looking back at the main page, it seems that the links to the transcript pages appear inside a `div` element class called `views-row`. If we look inside these `views-row` blocks, we should be able to extract the anchor `href` elements that point to the corresponding transcript links. Unfortunately, the structure of the page is set in such a way that only 12 `views-row` blocks are loaded at a time. The way to load a new set of blocks is to scroll down the page. This mechanism is called an _infinite scroll_ and is notoriously difficult to scrape. What we need to do in this case is to _keep scrolling down_ until we see the full set of links on the page. This is a tedious task to do manually, so we can use a package called `selenium`, which allows you to write programs that take control of your browser. Using `selenium`, we will open the site and continually press the Page Down key (<kbd>COMMAND</kbd>+<kbd>DOWN</kbd> on Mac OSX) until we've loaded the full set of transcript links. Once the full page is loaded, we can extract the `href` elements from all `views-row` blocks and create a list of links, which we will later individually parse using BeautifulSoup. \n",
    "\n",
    "Let's look at how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tell selenium that we will open the page using Safari.\n",
    "browser = webdriver.Safari()\n",
    "\n",
    "# Open the page in Safari.\n",
    "browser.get('https://millercenter.org/the-presidency/presidential-speeches')\n",
    "\n",
    "# Find the element that we need to interact with, in this case the 'body'.\n",
    "elem = browser.find_element_by_tag_name('body')\n",
    "\n",
    "# Specify the number of times we need to scroll down. 200 should be OK.\n",
    "no_of_pagedowns = 200\n",
    "\n",
    "# While we still need to scroll...\n",
    "while no_of_pagedowns: \n",
    "    \n",
    "    # Press COMMAND+DOWN to scroll down.\n",
    "    elem.send_keys(Keys.COMMAND+Keys.DOWN)\n",
    "    \n",
    "    # Wait a second so that the new set of links can load.\n",
    "    time.sleep(1.0)\n",
    "    \n",
    "    # De-increment the number of page-downs.\n",
    "    no_of_pagedowns-=1\n",
    "\n",
    "# Now that the entire page is loaded, extract all links in the block class \n",
    "post_links = browser.find_elements_by_css_selector(\".views-row a\")\n",
    "\n",
    "# Initialize an empty list to populate with links.\n",
    "links = []\n",
    "\n",
    "# For all anchors we found...\n",
    "for link in post_links: \n",
    "    \n",
    "    # Extract the 'href' element...\n",
    "    href = link.get_attribute('href')\n",
    "    \n",
    "    # ... and append its contents to the links list. \n",
    "    links.append(href)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the full list of links, we should probably save it so that we don't have to re-run the extraction script. We can do so using JSON, which is a syntax for storing data structures like lists, dictionaries, etc. This storing process is called *serialization*. When we serialize an object, we _encode_ it into a series of bytes that can later be _decoded_ or _deserialized_. Let's try to do this for our list in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"speech_links.json\", \"w\") as outfile:\n",
    "    json.dump(links, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our list is saved in the file named `speech_links.json`. If we'd like to de-serialize a JSON file and load it into a Python object, we can do so in a similar way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"links_output.json\", \"r\") as infile:\n",
    "    links = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code opens the JSON file, deserializes it, and loads it into a Python list object, which we can interact with. \n",
    "\n",
    "Now let's define a function that can take a link, parse its HTML and extract a cleaned version of the transcript text. We can use a dictionary to make it easy to search for presidents by their names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import unicodedata library to help with cleaning scraped text\n",
    "import unicodedata\n",
    "\n",
    "# Define a function that takes a miller center speech transcript link as an argument\n",
    "def get_speeches(link):\n",
    "    \n",
    "    # Get HTML from live web page\n",
    "    html_body = urllib.request.urlopen(link)\n",
    "    \n",
    "    # Parse HTML with BS\n",
    "    soup = BeautifulSoup(html_body,'html.parser')\n",
    "    \n",
    "    # Select appropriate element for transcript\n",
    "    transcript = soup.select('.transcript-inner p')\n",
    "    \n",
    "    # If the speech is captured on video, we need to instead click on \"View Transcript\"\n",
    "    if len(transcript)<=0:\n",
    "        transcript = soup.select('.view-transcript p')\n",
    "        \n",
    "    # Go through every <p> that BS finds, extract the text, and clean it\n",
    "    transcripts = [unicodedata.normalize(\"NFKD\", elem.get_text()) for elem in transcript]\n",
    "    \n",
    "    # Go through list of <p>'s and join them into a single string\n",
    "    speech = ' '.join(transcripts)\n",
    "    \n",
    "    # Get the speechgiver's name\n",
    "    president_name = soup.select('.president-name')[0].get_text()\n",
    "    speech_date = soup.select(\".episode-date\")[0].get_text()\n",
    "    # Create a dictionary consisting of two items: \n",
    "    # * the president's name\n",
    "    # * the president's speech\n",
    "    speech_dic = {'Name': president_name, 'Speech': speech, 'Date': speech_date}\n",
    "    \n",
    "    return speech_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Name': 'George Washington',\n",
       " 'Speech': 'Fellow Citizens of the Senate and the House of Representatives:  Among the vicissitudes incident to life, no event could have filled me with   greater anxieties than that of which the notification was transmitted by your   order, and received on the fourteenth day of the present month. On the one hand,   I was summoned by my Country, whose voice I can never hear but with veneration   and love, from a retreat which I had chosen with the fondest predilection, and,   in my flattering hopes, with an immutable decision, as the asylum of my declining   years: a retreat which was rendered every day more necessary as well as more   dear to me, by the addition of habit to inclination, and of frequent interruptions   in my health to the gradual waste committed on it by time. On the other hand,   the magnitude and difficulty of the trust to which the voice of my Country called   me, being sufficient to awaken in the wisest and most experienced of her citizens,   a distrustful scrutiny into his qualification, could not but overwhelm with   dispondence, one, who, inheriting inferior endowments from nature and unpractised   in the duties of civil administration, ought to be peculiarly conscious of his   own deficencies. In this conflict of emotions, all I dare aver, is, that it   has been my faithful study to collect my duty from a just appreciation of every   circumstance, by which it might be affected. All I dare hope, is, that, if in   executing this task I have been too much swayed by a grateful remembrance of   former instances, or by an affectionate sensibility to this transcendent proof,   of the confidence of my fellow-citizens; and have thence too little consulted   my incapacity as well as disinclination for the weighty and untried cares before   me; my error will be palliated by the motives which misled me, and its consequences   be judged by my Country, with some share of the partiality in which they originated.   Such being the impressions under which I have, in obedience to the public   summons, repaired to the present station; it would be peculiarly improper to   omit in this first official Act, my fervent supplications to that Almighty Being   who rules over the Universe, who presides in the Councils of Nations, and whose   providential aids can supply every human defect, that his benediction may consecrate   to the liberties and happiness of the People of the United States, a Government   instituted by themselves for these essential purposes: and may enable every   instrument employed in its administration to execute with success, the functions   allotted to his charge. In tendering this homage to the Great Author of every   public and private good, I assure myself that it expresses your sentiments not   less than my own; nor those of my fellow-citizens at large, less than either.   No People can be bound to acknowledge and adore the invisible hand, which conducts   the Affairs of men more than the People of the United States. Every step, by   which they have advanced to the character of an independent nation, seems to   have been distinguished by some token of providential agency. And in the important   revolution just accomplished in the system of their United Government, the tranquil   deliberations and voluntary consent of so many distinct communities, from which   the event has resulted, cannot be compared with the means by which most Governments   have been established, without some return of pious gratitude along with an   humble anticipation of the future blessings which the past seem to presage.   These reflections, arising out of the present crisis, have forced themselves   too strongly on my mind to be suppressed. You will join with me I trust in thinking,   that there are none under the influence of which, the proceedings of a new and   free Government can more auspiciously commence.   By the article establishing the Executive Department, it is made the duty   of the President \"to recommend to your consideration, such measures as   he shall judge necessary and expedient.\" The circumstances under which   I now meet you, will acquit me from entering into that subject, farther than   to refer to the Great Constitutional Charter under which you are assembled;   and which, in defining your powers, designates the objects to which your attention   is to be given. It will be more consistent with those circumstances, and far   more congenial with the feelings which actuate me, to substitute, in place of   a recommendation of particular measures, the tribute that is due to the talents,   the rectitude, and the patriotism which adorn the characters selected to devise   and adopt them. In these honorable qualifications, I behold the surest pledges,   that as on one side, no local prejudices, or attachments; no seperate views,   nor party animosities, will misdirect the comprehensive and equal eye which   ought to watch over this great assemblage of communities and interests: so,   on another, that the foundations of our National policy will be laid in the   pure and immutable principles of private morality; and the pre-eminence of a   free Government, be exemplified by all the attributes which can win the affections   of its Citizens, and command the respect of the world.   I dwell on this prospect with every satisfaction which an ardent love for   my Country can inspire: since there is no truth more thoroughly established,   than that there exists in the oeconomy and course of nature, an indissoluble   union between virtue and happiness, between duty and advantage, between the   genuine maxims of an honest and magnanimous policy, and the solid rewards of   public prosperity and felicity: Since we ought to be no less persuaded that   the propitious smiles of Heaven, can never be expected on a nation that disregards   the eternal rules of order and right, which Heaven itself has ordained: And   since the preservation of the sacred fire of liberty, and the destiny of the   Republican model of Government, are justly considered as deeply, perhaps as   finally staked, on the experiment entrusted to the hands of the American people.   Besides the ordinary objects submitted to your care, it will remain with your   judgment to decide, how far an exercise of the occasional power delegated by   the Fifth article of the Constitution is rendered expedient at the present juncture   by the nature of objections which have been urged against the System, or by   the degree of inquietude which has given birth to them. Instead of undertaking   particular recommendations on this subject, in which I could be guided by no   lights derived from official opportunities, I shall again give way to my entire   confidence in your discernment and pursuit of the public good: For I assure   myself that whilst you carefully avoid every alteration which might endanger   the benefits of an United and effective Government, or which ought to await   the future lessons of experience; a reverence for the characteristic rights   of freemen, and a regard for the public harmony, will sufficiently influence   your deliberations on the question how far the former can be more impregnably   fortified, or the latter be safely and advantageously promoted.   To the preceeding observations I have one to add, which will be most properly   addressed to the House of Representatives. It concerns myself, and will therefore   be as brief as possible. When I was first honoured with a call into the Service   of my Country, then on the eve of an arduous struggle for its liberties, the   light in which I contemplated my duty required that I should renounce every   pecuniary compensation. From this resolution I have in no instance departed.   And being still under the impressions which produced it, I must decline as inapplicable   to myself, any share in the personal emoluments, which may be indispensably   included in a permanent provision for the Executive Department; and must accordingly   pray that the pecuniary estimates for the Station in which I am placed, may,   during my continuance in it, be limited to such actual expenditures as the public   good may be thought to require.   Having thus imparted to you my sentiments, as they have been awakened by the   occasion which brings us together, I shall take my present leave; but not without   resorting once more to the benign parent of the human race, in humble supplication   that since he has been pleased to favour the American people, with opportunities   for deliberating in perfect tranquility, and dispositions for deciding with   unparellelled unanimity on a form of Government, for the security of their Union,   and the advancement of their happiness; so his divine blessing may be equally   conspicuous in the enlarged views, the temperate consultations, and the wise   measures on which the success of this Government must depend.',\n",
       " 'Date': 'April 30, 1789'}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_speeches(links[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's loop through our list of links and extract the speeches. We will save these dictionaries as single `.json` files in a directory called `us_presidential_speeches`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# speech_dir = \"./us_presidential_speeches\"\n",
    "\n",
    "# os.mkdir(speech_dir)\n",
    "\n",
    "for i, link in enumerate(links[::-1]):\n",
    "    speech_idx = str(i)\n",
    "    if len(speech_idx) == 1:\n",
    "        speech_idx = f\"00{speech_idx}\"\n",
    "    elif len(speech_idx) == 2:\n",
    "        speech_idx = f\"0{speech_idx}\"\n",
    "    else:\n",
    "        pass\n",
    "    with open(speech_dir+f\"/speech_{speech_idx}.json\", \"w\") as speech_out:\n",
    "        speech_dic = get_speeches(link)\n",
    "        json.dump(speech_dic, speech_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll leave off here. Next lecture, we'll see how we can work with this raw text, transform it into a numeric format, and visualize how different speeches may relate to each other quantitatively. \n",
    "\n",
    "Web scraping code largely adapted (stolen) from [here](https://github.com/hajir-almahdi/the-data-behind-presidental-charisma/blob/master/The-Data-Behind-Presidental-Charisma.ipynb). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
